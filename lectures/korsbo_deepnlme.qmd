---
title: "Deep nonlinear mixed effect models"
subtitle: Modeling longitudinal data with a mix of mechanism and machine-learning
engine: julia
execute:
  error: false
  daemon: 1800
  cache: true
  eval: false
  freeze: true
  echo: false
author:
  - Niklas Korsbo
affiliation: Pumas-AI
date: '2025-04-28'
julia:
  exeflags:
    - '--project=@.'
    - '-t 8'
logo: https://pumas-assets.s3.amazonaws.com/CompanyLogos/DeepPumas/RGB/SVG/DeepPumas+Primaryv.svg
format:
  revealjs:
    width: 1200
    height: 800
    auto-stretch: true
    # leave a bit of breathing room around the edges
    margin: 0.15

    # allow content to shrink to 30% (but never grow beyond 100%)
    min-scale: 0.3
    max-scale: 1.0
    # (optional) disable vertical centering if you’d rather top-align
    html-math-method: mathjax
    transition: none
    center: true
    scrollable: false
    progress: true
    slide-number: true
    smaller: false
    theme: 
      - company.scss 
      - deeppumas.scss 
    navigation-mode: vertical
    css: 
      - style.css
      - deeppumas-style.css
    mermaid:
      # theme: dark
      flowchart:
        htmlLabels: true
      # format: png
    mermaidConfig:
      flowchart:
        htmlLabels: true
    fig-align: center
    resources:
      - "fonts/AmplitudeBook.otf"
---


#


![&nbsp;](images/image.png){width=50% align="center"}


:::: {.columns style="font-size: 70%"}
::: {.column width=0.4}
Machine learning

::: {.callout-tip appearance="minimal"}
- Data-driven model discovery
- Finds unintuitive relationships
- Handles complex data
:::

::: {.callout-important appearance="minimal"}
- Lacks scientific understanding
- Requires big data
:::
:::
  

::: {.column width=0.4}
Scientific modelling

::: {.callout-tip appearance="minimal"}
- Encodes scientific understanding
- Data-efficient
- Interpretable
- Simple counterfactuals
:::

::: {.callout-important appearance="minimal"}
- Labor intensive
- Misses unintuitive relationships
- Hard to utilize complex data
:::
:::
::::
  


## Data+Knowledge {visibility="hidden"}
## {.center}

![](images/good_preds.svg)




## Vision spanning pharma {visibility="hidden"}
##

<!-- ![](image-6.png) -->
![](images/across_pharma.svg)


<!-- ## ~DeepNLME~

- SciML for longitudinal data
  - "Embedding" NNs (and friends) to identify functional relationships within scientific model structure.

Cool, but not the focus of this talk

- A different meaning of "Embedding"
  - Coming from generative AI
  - Extract machine-accessible information from complex data. -->


## 
```{julia}
#| warning: false
#| output: false
#| echo: false

using DeepPumas
using StableRNGs
using PumasPlots
using CairoMakie
using Serialization
using Latexify
using Markdown
using QuartoTools: @cache

assets = @__DIR__() * "/assets/"

DeepPumas.set_mlp_backend(:staticflux)
dark = false
if dark
  set_theme!(deep_dark(); backgroundcolor=:transparent, fontsize=45)
  fgcolor = (:white, 0.6)
else
  set_theme!(deep_light();backgroundcolor=:transparent, fontsize=45, Axis=(; 
    backgroundcolor=:transparent,
    # foregroundcolor=:black
  ))
  fgcolor = (:black, 0.7)
end

figure = (; resolution=(1200, 700), fontsize=25)

Latexify.set_default(starred=true, convert_unicode=false)
```


<!-- ![&nbsp;](image-7.png){width=70%} -->

<!-- ## -->

<!-- ![&nbsp;](image-1.png){width=70%} -->

```{julia}
#| output: false
pkmodel = @model begin
  @param begin
    tvKa in RealDomain(; lower=0)
    tvCL in RealDomain(; lower=0)
    tvVc in RealDomain(; lower=0)
    Ω in PDiagDomain(3)
    σ in RealDomain(; lower=0)
  end
  @random η ~ MvNormal(Ω)
  @pre begin
    Ka = tvKa * exp(η[1])
    CL = tvCL * exp(η[2])
    Vc = tvVc * exp(η[3])
  end
  @dynamics Depots1Central1
  @derived Concentration ~ @. Normal(Central/Vc, Central/Vc * σ)
end

param = (; tvKa = 2., tvCL=3., tvVc=1., Ω = Diagonal([0.5, 0.5, 0.5]), σ=0.1)
rng = StableRNG(2)
sims = map(1:20) do i 
  _subj = Subject(; id=i, events = DosageRegimen(1, ii=1, addl=2))
  sim = simobs(pkmodel, _subj, param; rng, obstimes = 0:0.2:4)
end
pop = Subject.(sims)
```

## {.smaller}

:::: {.columns}

::: {.column width="50%"}

**[Nonlinear Mixed Effects]{style="font-size: 170%"}**

::: {.fragment fragment-index=3 .fade-out}
::: {.absolute top=300, right=10, width=600}

```{julia}
sim = (; label="Data", markersize=15, linewidth=1, color=fgcolor)
plt = plotgrid(sims[1:1]; sim)
save(assets * "single_traj.png", plt; px_per_unit=4)
plt
```
![](assets/single_traj.png)

:::
:::

::: {.fragment fragment-index=3}

::: {.absolute top=500, right=100, width="600"}

```{julia}
foreach(sims[2:4]) do _sim
  plotgrid!(plt, [_sim]; sim)
end
save(assets * "multi_traj.png", plt; px_per_unit=4)
plt
```
![](assets/multi_traj.png)

:::
:::

:::

::: {.column width="50%"}


::: {.fragment fragment-index=5}
Typical values

$$
tvKa, \; tvCL, \; tvVc, \; Ω, \; σ
$$

Covariates
$$
Age, \; Weight
$$

Random effects

$$
η \sim MvNormal(Ω)
$$

:::
  
::: {.fragment fragment-index=4}
Individual parameters

\begin{align*}
Ka_i &= tvKa \cdot e^{η_{i,1}} \\
CL_i &= tvCL \cdot e^{η_{i,2}} \\
Vc_i &= tvVc \cdot e^{η_{i,3}}
\end{align*}
:::

::: {.fragment fragment-index=1}
Dynamics

$$
\begin{align*}
\frac{dDepot(t)}{dt} =&  - Ka \cdot Depot(t) \\
\frac{dCentral(t)}{dt} =& Ka \cdot Depot(t) - \frac{CL}{Vc} \cdot Central(t)
\end{align*}
$$


:::
::: {.fragment fragment-index=2}
Error model
$$
Concentration(t) \sim Normal\left(\frac{Central(t)}{Vc}, \frac{Central(t)}{Vc} \cdot σ\right)
$$
:::




:::
::::

---

```{julia}
#| output: false
using Flux
using CairoMakie
using Colors

ninput = 1
nhidden = 6
act=tanh

opt = Flux.Adam()
X = permutedims(collect(range(0, stop=1, length=301)))
Ys = [X .^ 2 .+ X, 2 .* X ./ (0.3 .+ X), sin.(2π .* X) .+ 1, exp2.(X)]

##
nnf = Flux.Chain(Dense(1,nhidden, act), Dense(nhidden, nhidden, act), Dense(nhidden, 1))
Ŷ = Observable(vec(Ys[end]))
Y = Observable(vec(Ys[1]))


fig, ax = lines(
  vec(X),
  Y; 
  linewidth=6,
  axis=(ylabel="Output", xlabel="x"),
  label="f(x)",
  figure=(
    ; size=(400,400),
    fontsize=25,
    # backgroundcolor=colorant"#002b36"
    backgroundcolor="white"
  )
)
lines!(vec(X), Ŷ, label="NN(x)", linewidth=6)
Legend(fig[0,:], ax, orientation=:horizontal, framevisible=false)
fig
Ys = [X .^ 2 .+ X, 2 .* X ./ (0.3 .+ X), sin.(2π .* X) .+ 1, exp2.(X)]
nframes = 400
opt_state = Flux.setup(opt, nnf)
record(fig, assets * "nn_demo_test.mp4", 1:nframes; framerate=30) do frame
  _Y = Ys[min(round(Int, frame ÷ (nframes/length(Ys))) + 1, end)]
  Y[] = vec(_Y)
  Ŷ[] = vec(nnf(X))
  
  steps_per_y = nframes / length(Ys)
  
  for j in 1:round(Int, 50 / steps_per_y * (frame%steps_per_y))
  grads = gradient(m -> Flux.mse(m(X), _Y), nnf)
    Flux.Optimise.update!(opt_state, nnf, grads[1])
  end
end 
```

## Neural networks {.smaller}

:::: {.columns}
::: {.column width="50%"}

[Information processing mechanism]{.att}

- Loosely based on neurons

![&nbsp;](image-4.png){fig-align="center"}

- Mathematically just a function!
- Usable anywhere you'd use a function!

:::
::: {.column width="50%"}

[Univeral approximators!]{.att}

![](assets/nn_demo_test.mp4){loop="true" autoplay="true" muted="true"}

- Approximate *any* function
- Functional form tuned by parameters

:::
::::

## {.smaller}

:::: {.columns}

::: {.column width="50%"}

**[Deep Nonlinear Mixed Effects]{.att style="font-size: 170%"}**

::: {.absolute top=500, right=100, width="600"}

![](assets/multi_traj.png)

:::
:::

::: {.column width="50%"}


Typical values

$$
tvKa, \; tvCL, \; tvVc, \; Ω, \; σ
$$

Covariates
$$
Age, \; Weight
$$

Random effects

$$
η \sim MvNormal(Ω)
$$

  
Individual parameters

\begin{align*}
Ka_i &= tvKa \cdot e^{η_{i,1}} \\
CL_i &= tvCL \cdot e^{η_{i,2}} \\
Vc_i &= tvVc \cdot e^{η_{i,3}}
\end{align*}

::: {.fragment}
![](image-8.png){.absolute top=50 right=90 width=110}
:::
::: {.fragment}
![](image-4.png){.absolute top=430 right=130 width=140}
:::
::: {.fragment}
![](image-4.png){.absolute bottom=140 right=120 width=160}
:::

Dynamics

$$
\begin{align*}
\frac{dDepot(t)}{dt} =&  - Ka \cdot Depot(t) \\
\frac{dCentral(t)}{dt} =& Ka \cdot Depot(t) - \frac{CL}{Vc} \cdot Central(t)
\end{align*}
$$

Error model
$$
Concentration(t) \sim Normal\left(\frac{Central(t)}{Vc}, \frac{Central(t)}{Vc} \cdot σ\right)
$$



:::
::::



# Neural-embedded dynamical systems {.smaller}

## Neural-embedded dynamical systems

2018 - "Neural Ordinary Differential Equations", Chen et al.

::: {.fragment fragment-index=1}
2020 - "Universal Differential Equations for Scientific Machine Learning", Rackauckas et al.
:::

:::: {.columns}
::: {.column width="50%"}

[Neural ODE]{.att}

$$
\frac{d\mathbf{X}}{dt} = NN(\mathbf{X}(t), t)
$$

::: {style="font-size: 80%"}
ODE solver as scaffold for neural networks

Similar to recurrent neural networks and ResNets

:::
:::
::: {.column width="50%"}

::: {.fragment fragment-index=1}
[Universal Differential Equations (UDE)]{.att}

\begin{align*}
\frac{dx}{dt} &= x \cdot y - NN(x)\\
\frac{dy}{dt} &= p - x \cdot y
\end{align*}

::: {style="font-size: 80%"}
Insert universal approximators (like NNs) to capture terms in dynamical systems. 
:::
:::

:::
<!-- ::: {.column width="33%"}
::: {.fragment fragment-index=2}

[Scientific Machine Learning (SciML)]{.att}

::: {style="font-size: 80%"}
An abstract concept of mixing scientific modeling with machine learning. 
:::

:::
::: -->
::::

## Encoded knowledge {.smaller}

:::: {.columns}

::: {.column width="50%"}

[
$$
\begin{aligned}
\frac{dDepot}{dt} &= NN(Depot, Central, R)[1]\\
\frac{dCentral}{dt} &= NN(Depot, Central, R)[2]\\
\frac{dR}{dt} &= NN(Depot, Central, R)[3]
\end{aligned}
$$
]{.att}

- Number of states

::: {.fragment}

[
$$
\begin{aligned}
\frac{dDepot}{dt} &= - NN_1(Depot)\\
\frac{dCentral}{dt} &= NN_1(Depot) - NN_2(Central)\\
\frac{dR}{dt} &= NN_3(Central, R)
\end{aligned}
$$
]{.att}

- Number of states
- Dependecies
- Conservation

:::
:::

::: {.column width="50%"}

::: {.fragment}

[
$$
\begin{aligned}
\frac{dDepot}{dt} &= - K_a \cdot Depot\\
\frac{dCentral}{dt} &= K_a \cdot Depot - CL/V_c \cdot Central\\
\frac{dR}{dt} &= NN_3\left(\frac{Central}{V_c}, R\right)
\end{aligned}
$$
]{.att}

- Explicit knowledge of some terms

:::
  
::: {.fragment}
[
$$
\begin{aligned}
\frac{dDepot}{dt} &= - K_a \cdot Depot\\
\frac{dCentral}{dt} &= K_a \cdot Depot - CL/V_c \cdot Central\\
\frac{dR}{dt} &= k_{in} \cdot \left(1 + NN\left(\frac{Central}{V_c}\right)\right) - k_{out} \cdot R
\end{aligned}
$$
]{.att}

- Precise position of the unknown function
- Precise input to the unknown input
- Lots of knowledge!

:::
:::


::::

## Extending for longitudinal data with DeepNLME {.smaller}


\begin{equation}
η \sim \mathcal{N}\left(Ω\right)
\end{equation}
\begin{align*}
Ka &= tvKa \cdot e^{η_{2}} \\
V_c &= tvV_c \cdot e^{η_{3}} \\
Kout &= tvKout \cdot e^{η_{4}}
\end{align*}

[
\begin{align*}
\frac{\mathrm{d} Depot(t)}{\mathrm{d}t} &=  - Ka \cdot Depot(t) \\
\frac{\mathrm{d} Central(t)}{\mathrm{d}t} &= \frac{ - CL \cdot Central(t)}{V_c} + Ka \cdot Depot(t) \\
\frac{\mathrm{d} R(t)}{\mathrm{d}t} &= Kin \cdot \left( 1 + NN\left(\frac{Central}{V_c} \right) \right) - Kout \cdot R(t)
\end{align*}
]{.att}

\begin{align*}
yPK &\sim \mathrm{Normal}\left( \frac{Central}{V_c}, σ_{pk} \right) \\
yPD &\sim \mathrm{Normal}\left( R, σ_{pd} \right)
\end{align*}


## Extending for longitudinal data with DeepNLME {.smaller}


\begin{equation}
η \sim \mathcal{N}\left(Ω\right)
\end{equation}
\begin{align*}
Ka &= tvKa \cdot e^{η_{2}} \\
V_c &= tvV_c \cdot e^{η_{3}} \\
Kout &= tvKout \cdot e^{η_{4}}
\end{align*}

\begin{align*}
\frac{\mathrm{d} Depot(t)}{\mathrm{d}t} &=  - Ka \cdot Depot(t) \\
\frac{\mathrm{d} Central(t)}{\mathrm{d}t} &= \frac{ - CL \cdot Central(t)}{V_c} + Ka \cdot Depot(t) \\
\frac{\mathrm{d} R(t)}{\mathrm{d}t} &= Kin \cdot \left( 1 + NN\left(\frac{Central}{V_c}, {\color{orange} η₁} \right) \right) - Kout \cdot R(t)
\end{align*}

\begin{align*}
yPK &\sim \mathrm{Normal}\left( \frac{Central}{V_c}, σ_{pk} \right) \\
yPD &\sim \mathrm{Normal}\left( R, σ_{pd} \right)
\end{align*}





```{julia}
#| eval: true
#| output: false
#| warning: false 
using AlgebraOfGraphics
using CairoMakie
using CSV
using DataFrames
using DataFramesMeta
using DeepPumas
using Flux
using Markdown
using MultivariateStats
using PairPlots
using PrettyTables
using PumasPlots
using Pumas.Latexify
using Random
using Tables
using Transformers
using Transformers.HuggingFace
using Transformers.TextEncoders
using QuartoTools
using QuartoTools: @cache
const AoG = AlgebraOfGraphics
set_theme!(deep_light(); backgroundcolor=:white)

assets = @__DIR__() * "/assets/"


"""
    unroll(nt::NamedTuple) -> NamedTuple

Transforms a NamedTuple by unrolling its array fields into individual key–value pairs. For each array field, each element is assigned a new key formed by appending a subscript (derived via `Pumas._to_subscript`) to the original field name. Non-array fields are kept unchanged.

# Examples
```julia
nt = (a = [10, 20], b = 5)
# unroll(nt) returns a NamedTuple with keys like :a₁, :a₂, and :b, for example:
# (:a₁ => 10, :a₂ => 20, :b => 5)
"""
function unroll(nt::NamedTuple)
  mapreduce(_unroll, merge, pairs(nt))
end

unroll(f::Function; kwargs...) = x->unroll(f, x; kwargs...)

_unroll(p::Pair) = _unroll(p.first, p.second)
_unroll(key::Symbol, val::Any) = (; key => val)
function _unroll(key::Symbol, val::AbstractArray{T}) where T
  inds = CartesianIndices(val)
  syms = Vector{Symbol}(undef, length(inds))
  flat_vals = Vector{T}(undef, length(inds))
  for (i, ind) ∈ enumerate(inds)
    syms[i] = Symbol(string(key, join(map(Pumas._to_subscript, Tuple(ind)), ",")))
    flat_vals[i] = val[ind]
  end
  NamedTuple{Tuple(syms)}(flat_vals)
end


# patient_data = CSV.read(@__DIR__() * "/data.csv", DataFrame);
patient_data = CSV.read(@__DIR__() * "/data_prognostic_text.csv", DataFrame);

pop = read_pumas(patient_data; observations = [:yPK, :yPD], covariates = [:Description, :Score])

train_pop = pop[1:100]
test_pop = pop[101:200]
```


```{julia}
#| echo: false
#| output: false
text_data = CSV.read(@__DIR__() * "/text_data.csv", DataFrame)

_pop = [Subject(; events = DosageRegimen(1), id=row.id, covariates = (; Score = row.Score, Description = row.Description)) for row in eachrow(text_data)]

datamodel = @model begin
    @param begin
        tvKa ∈ RealDomain()
        tvVc ∈ RealDomain()
        tvSmax ∈ RealDomain()
        tvSC50 ∈ RealDomain()
        tvKout ∈ RealDomain()
        Kin ∈ RealDomain()
        CL ∈ RealDomain()
        n ∈ RealDomain()
        Ω ∈ PDiagDomain(5)
        σ_pk ∈ RealDomain()
        σ_pd ∈ RealDomain()
    end
    @random η ~ MvNormal(Ω)
    @covariates Score
    @pre begin
        # s = (Score - 5) / 10
        Smax = tvSmax * exp(η[1]) + 3 * Score / 5.
        # Smax = tvSmax * exp((1-c) * η[1] + c * s) 
        # SC50 = tvSC50 * exp(η[2] + 0.3 * (Score / 5)^0.75)
        SC50 = tvSC50 * exp(η[2])
        Ka = tvKa * exp(η[3] + 0.3 * (Score/5)^2 )
        Vc = tvVc * exp(η[4])
        Kout = tvKout * exp(η[5] + 0.5 * Score/5)
    end
    @init R = Kin / Kout
    @vars begin
        cp = abs(Central / Vc)
        EFF = Smax * cp^n / (SC50^n + cp^n)
    end
    @dynamics begin
        Depot' = -Ka * Depot
        Central' = Ka * Depot - (CL / Vc) * Central
        R' = Kin * (1 + EFF) - Kout * R
    end
    @derived begin
        yPK ~ @. Normal(Central ./ Vc, σ_pk)
        yPD ~ @. Normal(R, σ_pd)
    end
end

data_params = (;
    tvKa = 0.5,
    tvVc = 1.0,
    tvSmax = 0.9,
    tvSC50 = 0.02,
    tvKout = 1.2,
    Kin = 1.2,
    CL = 1.0,
    n = 1.0,
    # Ω = I(5) * 1e-2,
    Ω = Diagonal(fill(0.02, 5)),
    σ_pk = 3e-2,
    σ_pd = 1e-1,
)

sim = simobs(datamodel, _pop, data_params; obstimes = vcat(0:0.5:3, 4:10.), rng = StableRNG(3))
pop= Subject.(sim)


train_pop = pop[1:100]
test_pop = pop[length(train_pop) .+ (1:100)]

pred_truth = predict(datamodel, test_pop, data_params; obstimes = 0:0.01:12)
plotgrid(pred_truth[1:12]; observation=:yPD)

# CSV.write(@__DIR__() * "/data_prognostic_text.csv", DataFrame(sim))


```

## Let's define such a model {.smaller}

```{julia}
#| echo: true
deepnlme_model = @model begin
    @param begin
        tvKa ∈ RealDomain(; lower=0.)
        tvVc ∈ RealDomain(; lower=0.)
        tvKout ∈ RealDomain(; lower=0.)
        Kin ∈ RealDomain(; lower=0.)
        CL ∈ RealDomain(; lower=0.)
        Ω ∈ PDiagDomain(5)
        σ_pk ∈ RealDomain(; lower=0.)
        σ_pd ∈ RealDomain(; lower=0.)
        NN ∈ MLPDomain(3, 4, 4, (1, identity); reg = L2(1e-2, bias=false))
    end
    @random η ~ MvNormal(Ω)
    @pre begin
        Ka = tvKa * exp(η[1])
        Vc = tvVc * exp(η[2])
        Kout = tvKout * exp(η[3])
    end
    @init R = Kin / Kout
    @dynamics begin
        Depot' = -Ka * Depot
        Central' = Ka * Depot - (CL / Vc) * Central
        R' = Kin * (1 + NN(Central / Vc, η[4], η[5])[1]) - Kout * R
    end
    @derived begin
        yPK ~ @. Normal(Central ./ Vc, σ_pk)
        yPD ~ @. Normal(R, σ_pd)
    end
end
```

## Fits well with data (N=100)


```{julia}
#| output: false
#| warning: false 
fpm = @cache fit(deepnlme_model, train_pop, init_params(deepnlme_model), MAP(FOCE()); optim_options=(; time_limit=4*60))
```

```{julia}
pred = predict(fpm, test_pop[1:6]; obstimes=0:0.025:12)
plt = plotgrid(pred; observation = :yPD, figure = (; size=(800, 500)), layout=(3,2))
save(assets * "deepnlme_ipred.png", plt; px_per_unit=4)
plt
```

![](assets/deepnlme_ipred.png)


(withheld subjects)


## Good fits

```{julia}
ins = inspect(fpm)
plt = goodness_of_fit(ins; observations=[:yPD], figure = (; size = (600,400), fontsize = 15))
save(assets * "deepnlme_gof.png", plt; px_per_unit=4)
plt
```


![](assets/deepnlme_gof)


































## The data


```{julia}
#| fig-align: center 
 
subj_ids = [1,2,5,6]
_df = @chain patient_data begin
  @by :id $first
  @select :id :Description :Score 
  _[subj_ids, :]
end

# display(_df) 

fig = data(DataFrame(groupby(patient_data, :id)[subj_ids])) * mapping(:time, :yPD, col=:id => presorted) * visual(Scatter) |> draw(; figure=(; size=(1000, 600)))
for (i, id) in enumerate(subj_ids)
  Label(fig.figure[2, i, Top()], _df[i, :Description], word_wrap=true, tellheight=false, fontsize=20, valign=:top)
  # Label(fig.figure[3, i], "Wellness score = $(_df[i, :Score])", tellwidth=false, word_wrap=true, fontsize=20)
end
Label(fig.figure[2, 0, Right()], "Baseline text\ncovariate", tellwidth=false, tellheight=false, word_wrap=true, fontsize=20, rotation=pi/2)

save(assets * "data_example.png", fig; px_per_unit=4)

fig

```

![](assets/data_example.png){width=80%}



N = `{julia} length(train_pop)`

## A simple NLME model

```{julia}
#| eval: true
#| echo: false
#| output: false
model = @model begin
    @param begin
        tvKa ∈ RealDomain(; lower=0.)
        tvVc ∈ RealDomain(; lower=0.)
        tvSmax ∈ RealDomain(; lower=0.)
        tvSC50 ∈ RealDomain(; lower=0.)
        tvKout ∈ RealDomain(; lower=0.)
        Kin ∈ RealDomain(; lower=0.)
        CL ∈ RealDomain(; lower=0.)
        Ω ∈ PDiagDomain(4)
        σ_pk ∈ RealDomain(; lower=0.)
        σ_pd ∈ RealDomain(; lower=0.)
    end
    @random η ~ MvNormal(Ω)
    @pre begin
        Smax = tvSmax * exp(η[1])
        SC50 = tvSC50 #* exp(η[2])
        Ka = tvKa * exp(η[2])
        Vc = tvVc * exp(η[3])
        Kout = tvKout * exp(η[4])
    end
    @init R = Kin / Kout
    @vars begin
        cp = abs(Central / Vc)
        EFF = Smax * cp / (SC50 + cp)
    end
    @dynamics begin
        Depot' = -Ka * Depot
        Central' = Ka * Depot - (CL / Vc) * Central
        R' = Kin * (1 + EFF) - Kout * R
    end
    @derived begin
        yPK ~ @. Normal(Central ./ Vc, σ_pk)
        yPD ~ @. Normal(R, σ_pd)
    end
end
```




```{julia}
#| eval: true
lstr = Latexify.LaTeXString("""
\\begin{equation}
η \\sim \\mathcal{N}\\left(Ω\\right)
\\end{equation}
$(latexify(model, :pre))
$(latexify(model, :dynamics))
$(latexify(model, :derived))
""")
```

## Good NLME posterior predictions (ipreds)

```{julia}
#| output: false
#| warning: false 
fitted_nlme = @cache fit(model, train_pop, init_params(model), FOCE())
pred = predict(fitted_nlme, test_pop[1:6]; obstimes=0:0.025:15)
```
```{julia}
plt = plotgrid(
  pred; 
  observation=:yPD,
  figure = (; size = (600, 400)),
  layout = (3,2),
)
save(assets * "ipred_traj.png", plt; px_per_unit=4)
plt
```

![](assets/ipred_traj)

## EBEs as patient embeddings



```{julia}
#| output: false
#| echo: false
η_train = @cache mapreduce(x->x.η, hcat, empirical_bayes(fitted_nlme))
η_test = @cache mapreduce(x->x.η, hcat, empirical_bayes(fitted_nlme, test_pop))
```




<!-- ::: {.column width="30%" .center} -->

<!-- [Input]{.center} -->

```{julia}
# fig = Figure()
pred = predict(fitted_nlme, test_pop[1:3])
# axes = [Axis(fig[i, 1]) for i in 1:3]
plt = plotgrid(
  pred; data=true,
  ipred=false,
  pred=false,
  figure = (; size = (300, 600)),
  # legend = (; nbanks = 3, orientation = :horizontal),
  legend=false,
  observation=:yPD,
  ylabel=""
)
save(assets * "encoder_1.png", plt; px_per_unit=4)
plt
```

<!-- ![](assets/encoder_1){width=100%} -->

<!-- ::: -->

<!-- ::: {.column width="40%" .center} -->

<!-- Embedding -->

```{julia}
# η_syms = [:η₁, :η₂, :η₃, :η₄] #, :η₅]
η_syms = [:η₁, :η₂, :η₃, :η₄, :η₅] #, :η₅]
ebes = getfield.(pred, :ebes)
_df = DataFrame(permutedims(reduce(hcat, only.(values.(ebes)))), η_syms)
patients = ["Patient $i" for i in 1:3]
_df = DataFrame(permutedims(reduce(hcat, only.(values.(ebes)))), η_syms)
@select! _df :patient =  ["Patient $i" for i in 1:3] All()

# _df1 = unstack(stack(_df, Not(:patient)), :patient, :value)
open(assets * "table_ebe.md", "w") do f
  pretty_table(f, _df; backend=Val(:markdown), formatters=ft_round(2), header=names(_df))
end
```





<!-- ::: {.small-text .center}
{{< include assets/table_ebe.md >}}
::: -->

<!-- ::: -->

<!-- ::: {.column width="30%"} -->

<!-- Recreation -->
```{julia}
sim = simobs(fitted_nlme.model, test_pop[1:3], coef(fitted_nlme), ebes; simulate_error=false)
plt = plotgrid(
  sim; 
  pred=false,
  sim = (; plot_type=:scatter),
  layout=(1,3),
  data=false,
  figure = (; size = (300, 600)),
  # legend = (; nbanks = 3, orientation = :horizontal),
  legend = false,
  ylabel="",
  observation=:yPD
)

save(assets * "encoder_2.png", plt; px_per_unit=4)
plt
```

:::: {layout="[30, 5, 30, 5, 30]" layout-valign="center" .center-h}

::: {#first-column .center-h}
**Data**

![](assets/encoder_1){width=100%}
:::

::: {}
:::


::: {}
**Embedding**

::: {.small-text .center}
{{< include assets/table_ebe.md >}}
:::
:::

::: {}
:::

::: {}
**Generated**

![](assets/encoder_2){width=100%}
:::

::::

<!-- :::: -->


## A quick cheat-check

Here, we know the "true" wellness score that's affecting the patient outcome


```{julia}
_df = @by DataFrame(predict(fitted_nlme)) :id $first
# η_syms = [:η₁, :η₂, :η₃, :η₄, :η₅]
plt = data(stack(_df, η_syms)) * mapping(:Score => "Latent wellness score", :value => "Empirical Bayes estimate", layout=:variable) * visual(Scatter) 
fig = draw(plt; figure = (; size=(600,400)))
save(assets * "ebe_vs_latent.png", fig; px_per_unit=4)
fig
```

![](assets/ebe_vs_latent){width=70% fig-align="center"}

## The ease of creating embeddings


```{julia}
#| echo: true
#| warning: false
# Load a pre-trained text embedding model from HuggingFace
loaded_model = hgf"avsolatorio/NoInstruct-small-Embedding-v0"

const encoder = loaded_model[1]
const llm = loaded_model[2];

```
```{julia}
# Define how to get a patient's embedding
get_embedding(subj::DeepPumas.Pumas.Subject) = get_embedding(subj.covariates(0).Description)
function get_embedding(context)
    enc = encode(encoder, context)
    out = llm(enc)
    return out.pooled
end

# Get the embeddings for all patients and put it in a matrix
X_train = mapreduce(get_embedding, hcat, train_pop)
X_test = mapreduce(get_embedding, hcat, test_pop)
```




## Embedding subspacing

- Consider the embedding space as a "meaning space"
- The original model (presumably) had Shakespear and twitter in it's training.
- Our data are all about describing wellness.
- Our data _should_ all be on a low-dimensional manifold of the embedding space.
  
PCA for low-loss dimension reduction.

## Embedding subspacing



```{julia}
trained_pca = fit(PCA, X_train; maxoutdim = 10)
X_train_pc = predict(trained_pca, X_train)
X_test_pc = predict(trained_pca, X_test)

scores = unique(patient_data, :id).Score


npc = 6
pc_labels = ["pc$(Pumas._to_subscript(i))" for i in 1:npc]
df = hcat(
  DataFrame(X_train_pc[1:npc, :]', pc_labels), 
  DataFrame((; wellness_score = scores[1:length(train_pop)])), 
)

plt = AoG.data(stack(df, pc_labels)) * mapping(:wellness_score => "Latent wellness", :value => "Principal component value"; layout = :variable) * visual(Scatter)
fig = draw(plt; figure = (; fontsize=16))

save(assets * "wellness_vs_latent.png", fig; px_per_unit=4)
fig
```

![](assets/wellness_vs_latent){width=60% fig-align="center"}

Use of "Latent wellness" is cheating. Informative, though.

## No-cheat subspace evaluation

```{julia}
npc = 4
pc_labels = ["pc$(Pumas._to_subscript(i))" for i in 1:6]
df = hcat(
    DataFrame(X_train_pc[1:npc, :]', pc_labels[1:npc]), 
    DataFrame(η_syms .=> eachrow(η_train))
)
plt = AoG.data(stack(stack(df, r"pc."; variable_name = :pc_sym, value_name=:pc_value), η_syms)) * mapping(:value => "Empirical Bayes estimate", :pc_value=>"Principal component value"; row = :pc_sym, col=:variable) * visual(Scatter)
fig = draw(plt; figure = (; fontsize=16, size = (800, 600)))

save(assets * "ebe_vs_embedding.png", fig; px_per_unit=4)
fig
```

![](assets/ebe_vs_embedding)

## Predicting posterior ηs


```{julia}

pc_embedder(subj::Pumas.Subject) = vec(predict(trained_pca, get_embedding(subj)))
pc_embedder(str::String) = vec(predict(trained_pca, get_embedding(str)))

_df = DataFrame(vcat(train_pop, test_pop))
_df = innerjoin(
  _df,
  DataFrame(embeddings = pc_embedder.(vcat(train_pop, test_pop)), id = getfield.(vcat(train_pop, test_pop), :id)); 
  on=:id
)

embedding_pop = read_pumas(_df; observations = [:yPK, :yPD], covariates = [:Description, :Score, :embeddings])

tpope = embedding_pop[1:length(train_pop)]
vpope = embedding_pop[length(train_pop)+1 : end]
```

```{julia}
target = preprocess(fitted_nlme.model, tpope, coef(fitted_nlme), FOCE(); covs=(:embeddings,))

nn = MLPDomain(numinputs(target), 15, 10, (numoutputs(target), identity); backend=:staticflux, act=tanh, reg=L1(1))
fnn = fit(nn, target; optim_options = (; loss = DeepPumas.l2), training_fraction=0.8)

## Plot the predictions of test data
vη = empirical_bayes(model, vpope, coef(fitted_nlme), FOCE())
id_nts = ((; id = i) for i in getfield.(vpope, :id))

df1 = stack(DataFrame(unroll.(merge.(fnn(vpope), id_nts))); value_name=:prediction)
df2 = stack(DataFrame(unroll.(merge.(vη, id_nts))); value_name = :target)
_df = innerjoin(df1, df2, on = [:id, :variable])

plt = data(_df) * mapping(:prediction, :target => "EBE (Test)", layout=:variable) 
fig = draw(plt)
save(assets * "predicted_ebes.png", fig; px_per_unit=4)
fig
```
```{julia}
using Luxor

nn

# 1. Define the MLP architecture
layer_sizes = [10, 15, 15, 4]   # 4 inputs → two hidden layers of 5 → 1 output

Drawing(800, 700, assets * "nn.png")
origin()
background("transparent")
r = 15
rel_x_space = 13
rel_y_space = 3
points = map(enumerate(range.(1, layer_sizes))) do (layer, node_range)
    map(node_range) do node
      Luxor.Point(
        r * rel_x_space * (layer - 0.5 - length(layer_sizes) / 2),
        r * rel_y_space * (node - 0.5 - layer_sizes[layer] / 2),
      )
    end
end
circles = map(x->circle.(x, r, :stroke), points)
edges = map(zip(points[1:end-1], points[2:end])) do (p1, p2)
  map(Iterators.product(p1, p2)) do (x,y)
    arrow(x + (r, 0), y - (r, 0); arrowheadangle = π/20 )
  end
end

Luxor.fontsize(45)
Luxor.text("Text embeddings", mean(points[1]) - (3r, 0), angle=-π/2, halign=:center)

map(enumerate(η_syms)) do (i, η)
  Luxor.text(string(η), points[end][i] + (2r, r/2), valign=:center, halign=:left)
end

finish()
preview()
```

::: {layout="[0.5,1]" layout-valign="center"}
![](assets/nn){width=100% fig-align="center"}

![](assets/predicted_ebes)
:::

## Usage with NLME

Multiple approaches:

-  Use embeddings as NLME covariates
-  Jointly model NLME and LLM latent spaces

Each with different assumptions and level of complexity available

## Embedding-augmented NLME predictions

```{julia}
using Setfield
@set! fitted_nlme.data = tpope
deep_fpm = @cache augment(fitted_nlme, fnn)

fit_deep = @cache fit(
  deep_fpm.model,
  tpope,
  coef(deep_fpm),
  MAP(FOCE());
  constantcoef = Tuple(setdiff(keys(coef(deep_fpm)), (:Ω,)))
)
```






```{julia}
pred = predict(deep_fpm, vpope[1:12]; obstimes = 0:0.1:10)

_dfp = DataFrame(predict(deep_fpm))
_dfp_orig = DataFrame(predict(fitted_nlme, vpope))
# _dfp_data = DataFrame(predict(datamodel, deep_fpm.data, data_params))
_dfp_data = DataFrame(predict(datamodel, vpope, data_params))
__df = @chain vcat(_dfp_orig, _dfp, _dfp_data, cols=:intersect, source = :source => [:Original, :Augmented, :DataModel]) begin
  dropmissing(:yPD)
  @by :source begin 
    :MAE = mean(abs, :yPD .- :yPD_pred)
    :r2 = cor(:yPD, :yPD_pred)^2
  end
end

open(assets * "aug.md", "w") do f
  pretty_table(f, __df; backend=Val(:markdown), formatters=ft_round(3), header=names(__df))
end
```




```{julia}
pred_data = predict(datamodel, vpope[1:12], data_params; obstimes = 0:0.1:10)
fig = plotgrid(pred; ipred=false, observation = :yPD)
plotgrid!(pred_data; ipred=false, pred=(; label = "DataModel", color=Cycled(3), linestyle=:dash), observation = :yPD)
save(assets * "aug_traj.png", fig; px_per_unit=4)
fig
```

:::: {layout="[20, 80]" layout-valign="center"}

::: {.small-text .center}
On withheld data

{{< include assets/aug.md >}}

:::

![](assets/aug_traj){width=80%}

::::


## Improved generative model
```{julia}
_vpc1 = @cache vpc(deep_fpm; observations=[:yPD])
fig_vpc_1 = vpc_plot(_vpc1; figure = (; size = (900,300)))
```

```{julia}
_vpc2 = @cache vpc(fit_deep; observations=[:yPD])
fig_vpc_2 = vpc_plot(_vpc2; figure = (; size = (900,300)))
```
```{julia}

save(assets * "vpc1.png", fig_vpc_1)

save(assets * "vpc2.png", fig_vpc_2)
```

Before

![](assets/vpc1){width=80%}

After

![](assets/vpc2){width=80%}

## Embeddings from any kind of data

-  Text
-  2D Images
-  3D Images (worse availability)
-  Omics (worse availability)

## Simple conceptual pipeline 

1. Model non-complex longitudinal data with NLME
1. Convert complex covariates to embeddings
1. Find relevant low-dimensional manifold using PCA
1. Regress processed embeddings to posterior NLME random effects
1. Use the regression model to predict EBEs, $η_{pred}$.
1. Use $η_{pred}$ as a covariate in the NLME model and add to the "real" random effect
$$
η \to η + η_{pred}
$$
 (for Normally distributed ηs)
1. Refit residual $η$ distribution to reflect less unexplained between-subject variability.
1. Do any "normal" NLME stuff. 
  
Close to trivial!


## Generalized conceptual pipeline

1. Train an NLME model on longitudinal clinical outcomes to
  - Minimize reconstruction loss: $\prod_i p(y_i | θ, η=η_i^*)$
  - Embed timecourses: $η_i^* = \operatorname{argmax}_{η_i} L(η_i | y_i, θ)$
  - All the while balancing the need for good reconstruction and robust embedding by maximizing
  $$
  \log p(y | θ) = \sum_i^N \log \int p(y_i | θ, η_i) \cdot p(η_i | θ) dη_i
  $$
  
## Generalized conceptual pipeline

2. Use a pre-trained embedding model to
  - Embed complex patient-specific data, $x_i$ as $z^* = \operatorname{argmax}_z L(z | x_i, w)$
3. By combining an ML (to help process $x_i$ into $z^*$) and an NLME model, maximize
$$
  \log p(y | θ, z = z^*) = \sum_i^N \log \int p(y_i | θ, η_i, z = z^*) \cdot p(η_i | θ) dη_i
$$








# Epidemiological example use-case

##

```{julia}
#| fig-cap: '&nbsp;'

t = 0:0.1:100

epi_data_model = @model begin
  @param begin
    σ ∈ RealDomain(; lower=0, init=10)
    tvc ∈ RealDomain(; lower=0)
    ω ∈ RealDomain(; lower=0)
  end
  @random η ~ Normal(0, ω)
  @pre begin
    c = tvc * exp(η)
    _λ = t -> c * (pdf.(Normal(16, 7),t) .+ 1.2.*pdf.(Normal(37, 10), t) .+ 0.4 .* pdf.(Normal(65, 14), t) .+ 0.02)
  end
  @init S = 1
  @dynamics begin
    S' = - _λ(t) * S
  end
  @derived begin
    "Positive test fraction"
    PosFrac ~ @. Beta(σ*(1-0.99S), σ*(0.99S))
  end
end


countries = ["Austria", "Belgium", "Bulgaria", "Croatia", "Cyprus", "Czech Republic", "Denmark", "Estonia", "Finland", "France", "Germany", "Greece", "Hungary", "Ireland", "Italy", "Latvia", "Lithuania", "Sweden"]

p_epi = (; tvc=0.3, σ=1000., ω=0.6)
obstimes=vcat(0:5,10:5:100)
rng = StableRNG(123)
sim = map(eachindex(countries)) do i
  simobs(
    epi_data_model,
    Subject(; id=countries[i]),
    p_epi;
    rng,
    obstimes = sort(sample(rng, obstimes, rand(rng, 1:20); replace=false))
  )
end
pop_epi_good = Subject.(sim)


sparse_countries = ["Brazil", "Vietnam"]
sim_sparse = map(eachindex(sparse_countries)) do i
  simobs(
    epi_data_model,
    Subject(; id=sparse_countries[i]),
    p_epi;
    obstimes = sort(sample(rng, 1:100, rand(rng, 1:2); replace=false)),
    rng
  )
end

sims = vcat(sim, sim_sparse)
pop_epi = Subject.(sims)

tpop_epi = pop_epi[1:end-4]
vpop_epi = pop_epi[end-3:end]

plt = plotgrid(
  pop_epi; 
  xlabel="AGE",
  ylabel="Fraction positive tests",
  figure = (; fontsize=20, resolution=0.7 .* (1700, 1200)),
  legend=false,
  data = (; markersize=10),
  axis = (; yticks=0:0.25:1),
  title = (s, i) -> "$(s.id) ($(s.id in getfield.(vpop_epi, :id) ? "Test" : "Train"))"
)
save(assets * "epi_data.png", plt; px_per_unit=4)
plt
```

![](assets/epi_data)

## 

```{julia}
#| echo: true
#| output: false
epi_model = @model begin
  @param begin
    N ∈ RealDomain(; lower=0, init=1000)
    λ ∈ MLPDomain(2, 5, 5, (1, softplus); reg=L2(1e-2))
  end
  @random η ~ Normal(0, 0.1)
  @init S = 1
  @dynamics begin
    S' = - λ(t/100, η)[1] * S
  end
  @derived PosFrac ~ @. Beta(abs(N*(1-0.99S)), abs(N*(0.99S)))
end
```

- Data-driven discovery of force-of-infection, $\lambda(Age)$.
- $\lambda$, is shared but tunable with a single degree of freedom across countries.

```{julia}
#| output: false
#| warning: false
fpm_epi = @cache fit(
  epi_model,
  tpop_epi,
  init_params(epi_model),
  MAP(LaplaceI());
  checkidentification=false,
  optim_options = (; time_limit=5*60)
)
```

##

```{julia}
#| fig-cap: '&nbsp;'
plt_epi = plotgrid(
  predict(fpm_epi, pop_epi; obstimes=1:100); 
  xlabel="AGE",
  ylabel="Fraction positive tests",
  figure = (; fontsize=20, resolution=0.7 .* (1700, 1200)),
  # legend=false,
  data = (; markersize=10),
  axis = (; yticks=0:0.25:1),
  title = (s, i) -> "$(s.id) ($(s.id in getfield.(vpop_epi, :id) ? "Test" : "Train"))",
  ipred = (; label="DeepNLME ipred"),
  pred=false,
)
save(assets * "epi_ipred.png", plt_epi; px_per_unit=4)
plt_epi
```

![](assets/epi_ipred.png)

##

```{julia}
#| fig-cap: '&nbsp;'
plotgrid!(
  plt_epi,
  simobs(epi_data_model, pop_epi, p_epi, getfield.(sims, :randeffs); simulate_error=false, obstimes=1:100); 
  sim = (; linestyle=:solid, markersize=0, linewidth=2, label="Truth")
)
save(assets * "epi_ipred_truth.png", plt_epi; px_per_unit=4)
plt_epi
```
![](assets/epi_ipred_truth.png)

## 

```{julia}
#| fig-cap: '&nbsp;'
country_id=20
nn = coef(fpm_epi).λ
η = empirical_bayes(fpm_epi, pop_epi[country_id]).η
inn = t -> nn(t/100, η)[1]
iλ = sims[country_id].icoefs._λ[1]
fig = Figure(; fontsize=30)
ax = Axis(fig[1,1], xlabel="Age", ylabel="λ", xticks=0:10:100, title=vcat(countries, sparse_countries)[country_id])
data_times = sims[country_id].time

lines!(ax, t, iλ; label="Truth")
scatter!(ax, data_times, iλ; markersize=25)
lines!(ax, t, inn; label="DeepNLME")
ylims!(ax, 0, nothing)
axislegend(ax)
fig
save(assets * "epi_foi.png", fig; px_per_unit=4)
fig
```
![](assets/epi_foi)

## What did this do?

```{julia}
ids = [1,2,19,20]
plotgrid(
  predict(fpm_epi, pop_epi[ids]; obstimes=1:100); 
  xlabel="AGE",
  ylabel="Fraction positive tests",
  figure = (; fontsize=30, resolution=0.7 .* (1700, 600)),
  # legend=false,
  data = (; markersize=15),
  axis = (; yticks=0:0.25:1),
  layout = (4,1),
  title = (s, i) -> "$(s.id) ($(s.id in getfield.(vpop_epi, :id) ? "Test" : "Train"))",
  ipred = (; label="DeepNLME ipred"),
  pred=false,
)
plotgrid!(
  simobs(epi_data_model, pop_epi[ids], p_epi, getfield.(sims[ids], :randeffs); simulate_error=false, obstimes=1:100); 
  xlabel="AGE",
  ylabel="Fraction positive tests",
  title = (s, i) -> "$(s.id) ($(s.id in getfield.(vpop_epi, :id) ? "Test" : "Train"))",
  sim = (; linestyle=:solid, markersize=0, linewidth=2, label="Truth")
)
```


- Disentangled differences from comonalities
- Enabled appropriate use of data of different quality
- Accurate predictions for countries with little data

# Generative AI

## Goal

To generate synthetic data that is indistinguishable in distribution from the real data.

![](images/observed_synthetic_contour.png){fig-align="center" width=60%}

## Images

<https://www.thispersondoesnotexist.com>

::::{.columns}

:::{.column width="50%"}
![](images/people_1.jpeg){width=90%}
:::

:::{.column width="50%"}
![](images/people_2.jpeg){width=90%}
:::

::::

## Time-series / longitudinal / panel data

<!-- re-generate these -->

::::{.columns}

:::{.column width="50%"}

![](images/pk_dist.png){width=100%}

:::

:::{.column width="50%"}

![](images/overview_vpc.png){width=100%}

:::

::::


## NLME hard to distinguish from GenAI

- Capture distribution of data along two levels
  - Unobserved (Latent variables)
  - Observed with observational noise (Conditional distibutions)
    
    
## 
    
![](images/vae_tutorial_arxiv.png){.img-border}

![](images/vae_tutorial_eq_1.png){.img-border}

![](images/vae_tutorial_eq_2.png){.img-border}

## NLME hard to distinguish from GenAI

NLME: maximize the marginal likelihood of our observations, $y$, conditional on covariates $c$ (including drug input), using fixed effects $\theta$:
$$
p_\theta(y | c) = \int p_\theta(y | \eta, c) \cdot p(\eta) d\eta
$$

Identical model goal, identical optimization target

From the VAE paper:

$$
p_\theta(x) = \int p_\theta(y | z) \cdot p(z) dz
$$

"Latent variables", $z$ are identical to random effects, $\eta$.


## What are these latent variables?

In NLME, their meaning is often structurally engineered
$$
CL = tvCL \cdot e^{\eta_1}
$$

But they don't have to be (DeepNLME) 
 
$$
\frac{dR}{dt} = NN\left(\frac{Central}{Vc}, R, \eta_1, \eta_2 \right)
$$

## What are these latent variables?
In GenAI, their meaning come from the training data and the training procedure

$$
p_\theta(x) = \int p_\theta(y | z) \cdot p(z) dz
$$

$z$ is a vector of informative data properties that are not directly observed.

## What are these latent variables?

For an image:

- Not pixel-by-pixel intensity

rather

- What objects are in the image
- What are the characteristics of the objects
- What are they doing
- What's the style
- ...

## What are these latent variables?
For text:

- Not words

rather:

- Sentiment
- Conveyed information
- Writing style
- Language
- ...

## Inferred latent variables as "Embeddings"

- The posterior latent variables tell you how to "recreate" the data.
- Fixed, and lower, dimensional represention of the data.
- Inferred latent variables, $z^*$, are called "Embeddings" and contain dense, neatly formatted, information about the data from which the embedding was inferred.
  
# Use-cases

##


![](images/bi_pics.png-1.png)

::: {layout="[[2,2], [1,3]]" layout-valign="center"}

![](images/bi_pic_1.png)

![](images/bi_pics.png-3.png)

![](images/bi_pics.png-5.png)

![](images/bi_pics.png-4.png)

:::

