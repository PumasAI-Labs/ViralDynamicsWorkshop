[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DeepPumas for Viral Dynamics Workshop",
    "section": "",
    "text": "This workshop explores how to combine mechanistic modeling with machine learning using DeepPumas for viral dynamics applications.\nTo follow along in the presentations, just navigate to any relevant bits from here.\n\n\n\nFire up a DeepPumas app in juliahub.com\n\nYou should have received a coupon code in your email. This code will give you credits and access to the DeepPumas app.\nWhen launching the DeepPumas app, use ‚Äúlaunch custom instance‚Äù and select 16vCPU and a 9h time limit.\n\nWait a minute for the VSCode view to pop up in your browser.\nClone this repository either\n\nIn the GUI\n\nBring up the command pallette (Ctrl+shift+p, or from the cogwheel in the bottom left corner)\nSearch for git clone\nclone https://github.com/PumasAI-Labs/ViralDynamicsWorkshop.git into /home/jrun/data/code/\n\nor, in the terminal terminal\n\nIf you don‚Äôt have a terminal, open one by searching for Terminal: focus on terminal view in the command pallette.\nNavigate to ~/data/code\nClone using git clone https://github.com/PumasAI-Labs/ViralDynamicsWorkshop.git"
  },
  {
    "objectID": "index.html#workshop-materials",
    "href": "index.html#workshop-materials",
    "title": "DeepPumas for Viral Dynamics Workshop",
    "section": "",
    "text": "This workshop explores how to combine mechanistic modeling with machine learning using DeepPumas for viral dynamics applications.\nTo follow along in the presentations, just navigate to any relevant bits from here.\n\n\n\nFire up a DeepPumas app in juliahub.com\n\nYou should have received a coupon code in your email. This code will give you credits and access to the DeepPumas app.\nWhen launching the DeepPumas app, use ‚Äúlaunch custom instance‚Äù and select 16vCPU and a 9h time limit.\n\nWait a minute for the VSCode view to pop up in your browser.\nClone this repository either\n\nIn the GUI\n\nBring up the command pallette (Ctrl+shift+p, or from the cogwheel in the bottom left corner)\nSearch for git clone\nclone https://github.com/PumasAI-Labs/ViralDynamicsWorkshop.git into /home/jrun/data/code/\n\nor, in the terminal terminal\n\nIf you don‚Äôt have a terminal, open one by searching for Terminal: focus on terminal view in the command pallette.\nNavigate to ~/data/code\nClone using git clone https://github.com/PumasAI-Labs/ViralDynamicsWorkshop.git"
  },
  {
    "objectID": "index.html#schedule-and-materials",
    "href": "index.html#schedule-and-materials",
    "title": "DeepPumas for Viral Dynamics Workshop",
    "section": "2 Schedule and Materials",
    "text": "2 Schedule and Materials\n\n\n\nTime\nSession\nMaterials\n\n\n\n\n09:00 - 09:20\nWelcome and Introduction\nSlides\n\n\n09:20 - 10:30\nNLME modeling in Pumas\nHands-on\n\n\n10:30 - 10:45\n‚òï Coffee Break\n\n\n\n10:45 - 11:15\nDeepNLME\nSlides\n\n\n\nNeural networks, SciML, UDEs and NeuralODEs\n\n\n\n11:15 - 12:30\nDeepNLME (hands-on)\nHands-on\n\n\n12:30 - 13:30\nüçΩÔ∏è Lunch Break\n\n\n\n13:30 - 13:55\nRandom effects, fitting NLME, and Generative AI\nSlides\n\n\n13:55 - 14:40\nDeepNLME with Complex Covariates\nSlides and hands-on\n\n\n14:40 - 15:00\nNLME and Generative AI\nSlides\n\n\n15:00 - 15:30\nEmbeddings\nSlides or hands-on\n\n\n15:30 - 15:45\n‚òï Coffee Break\n\n\n\n15:45 - 16:15\nEpidemiology Demo\nSlides\n\n\n16:15 - 17:00\nDiscussions and Conclusions"
  },
  {
    "objectID": "index.html#key-concepts-covered",
    "href": "index.html#key-concepts-covered",
    "title": "DeepPumas for Viral Dynamics Workshop",
    "section": "3 Key Concepts Covered",
    "text": "3 Key Concepts Covered\n\n3.1 Introduction\n\nComparison of machine learning vs scientific modeling approaches\nThe vision for combining both paradigms\nWorkshop overview and objectives\n\n\n\n3.2 DeepNLME - Part 1\n\nNeural networks as universal approximators\nScientific Machine Learning (SciML) and Universal Differential Equations (UDEs)\nNeural ODEs vs structured neural enhancement\nExtending UDEs for longitudinal data with NLME\n\n\n\n3.3 Complex Covariates\n\nPatient embeddings and Empirical Bayes Estimates (EBEs)\nText embeddings using pre-trained models\nDimensionality reduction and subspace analysis\nNeural networks for predicting patient parameters\nAugmented NLME models\n\n\n\n3.4 Epidemiology Demo\n\nAge-stratified seroprevalence modeling\nData-driven discovery of force of infection patterns\nInformation sharing across populations\nHandling heterogeneous data quality\n\n\n\n3.5 Generative AI Connections\n\nMathematical connections between NLME and Generative AI\nLatent variables, embeddings, and their clinical interpretations\nFuture directions and applications\nDiscussion of regulatory and practical considerations"
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "DeepPumas for Viral Dynamics Workshop",
    "section": "4 Learning Objectives",
    "text": "4 Learning Objectives\nBy the end of this workshop, participants will be able to:\n\nUnderstand the theoretical foundations connecting mechanistic modeling and machine learning\nImplement DeepNLME models for viral dynamics and other applications\n\nWork with complex covariates including text data and embeddings\nApply information sharing techniques across populations\nRecognize connections between NLME and modern generative AI\nEvaluate when and how to use DeepNLME approaches in practice"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "DeepPumas for Viral Dynamics Workshop",
    "section": "5 Prerequisites",
    "text": "5 Prerequisites\n\nFamiliarity with NLME modeling concepts"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "DeepPumas for Viral Dynamics Workshop",
    "section": "6 Resources",
    "text": "6 Resources\n\nDeepPumas Documentation\nPumas Documentation\nWorkshop GitHub Repository"
  },
  {
    "objectID": "06-embeddings.html#the-data-challenge",
    "href": "06-embeddings.html#the-data-challenge",
    "title": "DeepNLME with Complex Covariates",
    "section": "The Data Challenge",
    "text": "The Data Challenge\n\nChallenge: How do we use rich text descriptions as covariates in NLME models?"
  },
  {
    "objectID": "06-embeddings.html#traditional-nlme-with-simple-covariates",
    "href": "06-embeddings.html#traditional-nlme-with-simple-covariates",
    "title": "DeepNLME with Complex Covariates",
    "section": "Traditional NLME with Simple Covariates",
    "text": "Traditional NLME with Simple Covariates\n\nGood predictions, but missing information from complex covariates"
  },
  {
    "objectID": "06-embeddings.html#ebes-as-patient-embeddings",
    "href": "06-embeddings.html#ebes-as-patient-embeddings",
    "title": "DeepNLME with Complex Covariates",
    "section": "EBEs as Patient Embeddings",
    "text": "EBEs as Patient Embeddings\n\n\n\nData\n\n\n\n\n\n\nEmbedding\n\n\n\n\npatient\nŒ∑‚ÇÅ\nŒ∑‚ÇÇ\nŒ∑‚ÇÉ\nŒ∑‚ÇÑ\n\n\n\n\nPatient 1\n0.42\n0.1\n-0.26\n0.28\n\n\nPatient 2\n0.66\n0.08\n-0.92\n0.23\n\n\nPatient 3\n-0.43\n-0.03\n0.23\n-0.28\n\n\n\n\n\n\n\n\n\nGenerated\n\n\n\n\nKey insight: EBEs encode patient-specific information as low-dimensional vectors!"
  },
  {
    "objectID": "06-embeddings.html#validation-known-relationships",
    "href": "06-embeddings.html#validation-known-relationships",
    "title": "DeepNLME with Complex Covariates",
    "section": "Validation: Known Relationships",
    "text": "Validation: Known Relationships\n\n\n\n\n\nEBEs capture the underlying patient characteristics!"
  },
  {
    "objectID": "06-embeddings.html#the-ease-of-creating-embeddings",
    "href": "06-embeddings.html#the-ease-of-creating-embeddings",
    "title": "DeepNLME with Complex Covariates",
    "section": "The Ease of Creating Embeddings",
    "text": "The Ease of Creating Embeddings\n\n# Load a pre-trained text embedding model from HuggingFace\nloaded_model = hgf\"avsolatorio/NoInstruct-small-Embedding-v0\"\n\nconst encoder = loaded_model[1]\nconst llm = loaded_model[2];\n\n# Define how to get a patient's embedding\nget_embedding(subj::DeepPumas.Pumas.Subject) = get_embedding(subj.covariates(0).Description)\nfunction get_embedding(context)\n    enc = encode(encoder, context)\n    out = llm(enc)\n    return out.pooled\nend\n\n# Get the embeddings for all patients and put it in a matrix\nX_train = mapreduce(get_embedding, hcat, train_pop)\nX_test = mapreduce(get_embedding, hcat, test_pop)"
  },
  {
    "objectID": "06-embeddings.html#embedding-subspacing",
    "href": "06-embeddings.html#embedding-subspacing",
    "title": "DeepNLME with Complex Covariates",
    "section": "Embedding Subspacing",
    "text": "Embedding Subspacing\n\nConsider the embedding space as a ‚Äúmeaning space‚Äù\nThe original model had Shakespeare and Twitter in its training\nOur data are all about describing wellness\nOur data should be on a low-dimensional manifold of the embedding space\n\nSolution: Use PCA for dimension reduction\n\ntrained_pca = fit(PCA, X_train; maxoutdim = 10)\nX_train_pc = predict(trained_pca, X_train)\nX_test_pc = predict(trained_pca, X_test)"
  },
  {
    "objectID": "06-embeddings.html#embedding-space-structure",
    "href": "06-embeddings.html#embedding-space-structure",
    "title": "DeepNLME with Complex Covariates",
    "section": "Embedding Space Structure",
    "text": "Embedding Space Structure\n\n\n\n\n\nText embeddings capture meaningful clinical information!"
  },
  {
    "objectID": "06-embeddings.html#connecting-embeddings-to-ebes",
    "href": "06-embeddings.html#connecting-embeddings-to-ebes",
    "title": "DeepNLME with Complex Covariates",
    "section": "Connecting Embeddings to EBEs",
    "text": "Connecting Embeddings to EBEs\n\nStrong correlations between text embeddings and patient-specific parameters!"
  },
  {
    "objectID": "06-embeddings.html#neural-network-architecture",
    "href": "06-embeddings.html#neural-network-architecture",
    "title": "DeepNLME with Complex Covariates",
    "section": "Neural Network Architecture",
    "text": "Neural Network Architecture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText ‚Üí Embeddings ‚Üí Neural Network ‚Üí Patient Parameters"
  },
  {
    "objectID": "06-embeddings.html#usage-with-nlme",
    "href": "06-embeddings.html#usage-with-nlme",
    "title": "DeepNLME with Complex Covariates",
    "section": "Usage with NLME",
    "text": "Usage with NLME\nMultiple approaches available:\n\nUse embeddings as NLME covariates\n\nStraightforward integration\nLimited by traditional covariate modeling\n\nJointly model NLME and embeddings\n\nMore sophisticated approach\nAccounts for uncertainty in embeddings\n\nAugment existing models\n\nAdd predicted EBEs to trained models\nPreserve existing model structure"
  },
  {
    "objectID": "06-embeddings.html#embedding-augmented-predictions",
    "href": "06-embeddings.html#embedding-augmented-predictions",
    "title": "DeepNLME with Complex Covariates",
    "section": "Embedding-Augmented Predictions",
    "text": "Embedding-Augmented Predictions\n\n\n\nPerformance on test data\n\n\n\nsource\nMAE\nr2\n\n\n\n\nOriginal\n0.361\n0.712\n\n\nAugmented\n0.27\n0.82\n\n\nDataModel\n0.229\n0.865\n\n\n\n\n\n\n\n\n\nSignificant improvement using text embeddings!"
  },
  {
    "objectID": "06-embeddings.html#improved-population-modeling",
    "href": "06-embeddings.html#improved-population-modeling",
    "title": "DeepNLME with Complex Covariates",
    "section": "Improved Population Modeling",
    "text": "Improved Population Modeling\nBefore (traditional NLME) \nAfter (embedding-augmented) \nBetter population-level predictions and reduced unexplained variability!"
  },
  {
    "objectID": "06-embeddings.html#embeddings-from-any-data",
    "href": "06-embeddings.html#embeddings-from-any-data",
    "title": "DeepNLME with Complex Covariates",
    "section": "Embeddings from Any Data",
    "text": "Embeddings from Any Data\n\nText (clinical notes, patient descriptions)\n2D Images (X-rays, histology, photos)\n3D Images (CT scans, MRI - limited model availability)\n\nOmics data (genomics, proteomics - specialized models)\nTime series (ECG, continuous monitoring)\nMixed modalities (combining multiple data types)"
  },
  {
    "objectID": "06-embeddings.html#simple-conceptual-pipeline",
    "href": "06-embeddings.html#simple-conceptual-pipeline",
    "title": "DeepNLME with Complex Covariates",
    "section": "Simple Conceptual Pipeline",
    "text": "Simple Conceptual Pipeline\n\nModel longitudinal data with traditional NLME\nConvert complex covariates to embeddings using pre-trained models\nFind relevant subspace using dimensionality reduction (PCA)\nRegress embeddings to EBEs using neural networks\nPredict patient parameters for new subjects: \\(Œ∑_{pred}\\)\nAugment NLME model: \\(Œ∑ \\rightarrow Œ∑ + Œ∑_{pred}\\)\nRefit residual distribution to reflect reduced unexplained variability\nApply standard NLME methods (VPC, simulations, etc.)\n\nClose to trivial to implement!"
  },
  {
    "objectID": "06-embeddings.html#generalized-framework",
    "href": "06-embeddings.html#generalized-framework",
    "title": "DeepNLME with Complex Covariates",
    "section": "Generalized Framework",
    "text": "Generalized Framework\nTraining Phase: \\[\n\\max_Œ∏ \\log p(y | Œ∏) = \\sum_i^N \\log \\int p(y_i | Œ∏, Œ∑_i) \\cdot p(Œ∑_i | Œ∏) dŒ∑_i\n\\]\nEmbedding Phase:\n\\[\nz_i^* = \\text{Embed}(x_i) \\quad \\text{where } x_i \\text{ is complex covariate data}\n\\]\nJoint Modeling: \\[\n\\max_{Œ∏,w} \\log p(y | Œ∏, z^*) = \\sum_i^N \\log \\int p(y_i | Œ∏, Œ∑_i, z_i^*) \\cdot p(Œ∑_i | Œ∏) dŒ∑_i\n\\]"
  },
  {
    "objectID": "06-embeddings.html#key-advantages",
    "href": "06-embeddings.html#key-advantages",
    "title": "DeepNLME with Complex Covariates",
    "section": "Key Advantages",
    "text": "Key Advantages\n\nLeverages existing models: Use pre-trained embeddings (GPT, BERT, vision models)\nData efficient: Much less data needed than training from scratch\nScientifically grounded: Preserves mechanistic understanding in NLME\nFlexible: Works with any type of complex covariate\nInterpretable: Can understand which parts are mechanism vs data-driven\nScalable: Can handle large-scale clinical datasets"
  },
  {
    "objectID": "06-embeddings.html#next-steps",
    "href": "06-embeddings.html#next-steps",
    "title": "DeepNLME with Complex Covariates",
    "section": "Next Steps",
    "text": "Next Steps\n\nComing up: Hands-on implementation\n\nWork with synthetic text data\nBuild embedding pipelines\n\nAugment NLME models\nSee the power of complex covariates in action"
  },
  {
    "objectID": "04-generative-ai.html#generative-ai",
    "href": "04-generative-ai.html#generative-ai",
    "title": "Generative AI and NLME",
    "section": "Generative AI",
    "text": "Generative AI\nGoal: Generate data indistinguishable in distribution to real data"
  },
  {
    "objectID": "04-generative-ai.html#section",
    "href": "04-generative-ai.html#section",
    "title": "Generative AI and NLME",
    "section": "",
    "text": "~ \nwww.thispersondoesnotexist.com"
  },
  {
    "objectID": "04-generative-ai.html#how-is-that-done",
    "href": "04-generative-ai.html#how-is-that-done",
    "title": "Generative AI and NLME",
    "section": "How is that done?",
    "text": "How is that done?\n\n\nData is a mix of\n\nObserved quantities (pixel intensities)\nUnobserved quantities (faces, smiling, ‚Ä¶)\n\nWe humans learn to extract the unobserved quantities\nGenAI needs to do that too."
  },
  {
    "objectID": "04-generative-ai.html#how-is-that-done-1",
    "href": "04-generative-ai.html#how-is-that-done-1",
    "title": "Generative AI and NLME",
    "section": "How is that done?",
    "text": "How is that done?\n\n\nData is a mix of\n\nObserved quantities (\\(y\\))\nUnobserved quantities (\\(z\\) - ‚Äúlatent variables‚Äù)"
  },
  {
    "objectID": "04-generative-ai.html#generative-models",
    "href": "04-generative-ai.html#generative-models",
    "title": "Generative AI and NLME",
    "section": "Generative models",
    "text": "Generative models\n\nDefinitions\n\n\\(z\\): latent variables of dimension \\(d\\)\n\\(y\\): observed data\n\\(y_g\\): generated/simulated/synthetic data\n\nModel\n\n\\[\n\\begin{aligned}\ny_g &= f(z) + \\epsilon \\\\\nz &\\sim Normal(0, I_{d\\times d}) \\\\\n\\epsilon &\\sim Normal(0, \\sigma^2)\n\\end{aligned}\n\\]\n\nObjective: find \\(f\\) such that the distribution of \\(y_g\\) is close to the distribution of the observed data \\(y\\)"
  },
  {
    "objectID": "04-generative-ai.html#nlme-is-generative-ai",
    "href": "04-generative-ai.html#nlme-is-generative-ai",
    "title": "Generative AI and NLME",
    "section": "NLME is Generative AI!",
    "text": "NLME is Generative AI!\n\nDefinitions\n\n\\(\\eta\\): latent variables of dimension \\(d\\) and covariance matrix \\(\\Omega\\)\n\\(x\\): observed covariates\n\\(dv\\): observed data/response\n\\(dv_g\\): generated/simulated/synthetic data\n\nModel\n\n\\[\n\\begin{aligned}\ndv_g &= f_\\theta(\\eta, x) + \\epsilon \\\\\n\\eta &\\sim Normal(0, \\Omega) \\\\\n\\epsilon &\\sim Normal(0, \\sigma^2)\n\\end{aligned}\n\\]\n\nObjective: find \\(f\\) such that the conditional distribution of \\(dv_g | x\\) is close to the distribution of the observed data \\(dv\\)"
  },
  {
    "objectID": "04-generative-ai.html#nlme-is-generative-ai-1",
    "href": "04-generative-ai.html#nlme-is-generative-ai-1",
    "title": "Generative AI and NLME",
    "section": "NLME is Generative AI!",
    "text": "NLME is Generative AI!\nNLME objective: Maximize marginal likelihood of observations \\(y\\) given covariates \\(c\\):\n\\[\np_\\theta(y | c) = \\int p_\\theta(y | \\eta, c) \\cdot p(\\eta) d\\eta\n\\]\nVariational Autoencoder (GenAI) objective: Maximize likelihood of data \\(x\\):\n\\[\np_\\theta(x) = \\int p_\\theta(x | z) \\cdot p(z) dz\n\\]\n\nThey‚Äôre identical!\n\nRandom effects \\(\\eta\\) ‚ÜîÔ∏é Latent variables \\(z\\)\nObservations \\(y\\) ‚ÜîÔ∏é Generated data \\(x\\)\nIndividual predictions ‚ÜîÔ∏é Generative model"
  },
  {
    "objectID": "04-generative-ai.html#generative-ai-typical-anatomy",
    "href": "04-generative-ai.html#generative-ai-typical-anatomy",
    "title": "Generative AI and NLME",
    "section": "Generative AI ‚Äì Typical anatomy",
    "text": "Generative AI ‚Äì Typical anatomy"
  },
  {
    "objectID": "04-generative-ai.html#nlme-as-genai",
    "href": "04-generative-ai.html#nlme-as-genai",
    "title": "Generative AI and NLME",
    "section": "NLME as GenAI",
    "text": "NLME as GenAI\n\n\n\nInput: Time series\nOutput: Time series\n\n¬†\n\nDecoder: The ‚Äústructural‚Äù NLME model\nEncoder: an inverse problem of the decoder"
  },
  {
    "objectID": "04-generative-ai.html#what-are-latent-variables",
    "href": "04-generative-ai.html#what-are-latent-variables",
    "title": "Generative AI and NLME",
    "section": "What Are Latent Variables?",
    "text": "What Are Latent Variables?\nIn Traditional NLME: Meaning is structurally engineered\n\\[\nCL = tvCL \\cdot e^{\\eta_1}\n\\]\nIn DeepNLME: More flexible, but still some structure\n\\[\n\\frac{dR}{dt} = NN\\left(\\frac{Central}{Vc}, R, \\eta_1, \\eta_2 \\right)\n\\]\nIn Pure GenAI: Meaning emerges from data and training\n\\[\np_\\theta(x) = \\int p_\\theta(x | z) \\cdot p(z) dz\n\\]\n\\(z\\) captures informative properties not directly observed"
  },
  {
    "objectID": "04-generative-ai.html#latent-variables-as-information",
    "href": "04-generative-ai.html#latent-variables-as-information",
    "title": "Generative AI and NLME",
    "section": "Latent Variables as Information",
    "text": "Latent Variables as Information\nFor images:\n\nNot pixel-by-pixel intensity\nRather: Objects, characteristics, actions, style, lighting\n\nFor text:\n\nNot individual words\nRather: Sentiment, information content, writing style, language\n\nFor clinical data:\n\nNot individual measurements\nRather: Disease state, treatment response, patient phenotype"
  },
  {
    "objectID": "02-deepnlme-part1.html#traditional-nlme",
    "href": "02-deepnlme-part1.html#traditional-nlme",
    "title": "DeepNLME",
    "section": "Traditional NLME",
    "text": "Traditional NLME\n\n\nNonlinear Mixed Effects\n\n\n\n\nTypical values \\[\ntvKa, \\; tvCL, \\; tvVc, \\; Œ©, \\; œÉ\n\\]\nCovariates \\[\nAge, \\; Weight\n\\]\nRandom effects \\[\nŒ∑ \\sim MvNormal(Œ©)\n\\]\nIndividual parameters \\[\\begin{align*}\nKa_i &= tvKa \\cdot e^{Œ∑_{i,1}} \\\\\nCL_i &= tvCL \\cdot e^{Œ∑_{i,2}} \\\\\nVc_i &= tvVc \\cdot e^{Œ∑_{i,3}}\n\\end{align*}\\]\nDynamics \\[\n\\begin{align*}\n\\frac{dDepot(t)}{dt} =&  - Ka \\cdot Depot(t) \\\\\n\\frac{dCentral(t)}{dt} =& Ka \\cdot Depot(t) - \\frac{CL}{Vc} \\cdot Central(t)\n\\end{align*}\n\\]\nError model \\[\ndv(t) \\sim Normal\\left(\\frac{Central(t)}{Vc}, \\frac{Central(t)}{Vc} \\cdot œÉ\\right)\n\\]"
  },
  {
    "objectID": "02-deepnlme-part1.html#what-if-components-are-unknown",
    "href": "02-deepnlme-part1.html#what-if-components-are-unknown",
    "title": "DeepNLME",
    "section": "What if components are unknown?",
    "text": "What if components are unknown?\n\n\nNonlinear Mixed Effects\n\n\n\n\nTypical values \\[\ntvKa, \\; tvCL, \\; tvVc, \\; Œ©, \\; œÉ\n\\]\nCovariates \\[\nAge, \\; Weight, \\; \\color{red}{???}\n\\]\nRandom effects \\[\nŒ∑ \\sim MvNormal(Œ©)\n\\]\nIndividual parameters \\[\\begin{align*}\nKa_i &= tvKa \\cdot e^{Œ∑_{i,1}} \\\\\nCL_i &= tvCL \\cdot e^{Œ∑_{i,2}} \\cdot \\color{red}{???} \\\\\nVc_i &= tvVc \\cdot e^{Œ∑_{i,3}}\n\\end{align*}\\]\nDynamics \\[\n\\begin{align*}\n\\frac{dDepot(t)}{dt} =&  - Ka \\cdot Depot(t) \\\\\n\\frac{dCentral(t)}{dt} =& Ka \\cdot Depot(t) - \\color{red}{???}\n\\end{align*}\n\\]\nError model \\[\ndv(t) \\sim Normal\\left(\\frac{Central(t)}{Vc}, \\frac{Central(t)}{Vc} \\cdot œÉ\\right)\n\\]"
  },
  {
    "objectID": "02-deepnlme-part1.html#neural-networks",
    "href": "02-deepnlme-part1.html#neural-networks",
    "title": "DeepNLME",
    "section": "Neural networks",
    "text": "Neural networks\n\n\nInformation processing mechanism\n\nLoosely based on neurons\n\n\n\n\n\n\n\n\n\n¬†\n\n\n\nMathematically just a function!\nUsable anywhere you‚Äôd use a function!\n\n\nUniversal approximators!\nVideo\n\nApproximate any function\nFunctional form tuned by parameters"
  },
  {
    "objectID": "02-deepnlme-part1.html#neural-networks-in-nlme-dynamics",
    "href": "02-deepnlme-part1.html#neural-networks-in-nlme-dynamics",
    "title": "DeepNLME",
    "section": "Neural networks in NLME dynamics",
    "text": "Neural networks in NLME dynamics\n\n\nFocus: Neural Networks in Dynamics\n\n\n\n\nTypical values \\[\ntvKa, \\; tvCL, \\; tvVc, \\; Œ©, \\; œÉ\n\\]\nCovariates \\[\nAge, \\; Weight\n\\]\nRandom effects \\[\nŒ∑ \\sim MvNormal(Œ©)\n\\]\nIndividual parameters \\[\\begin{align*}\nKa_i &= tvKa \\cdot e^{Œ∑_{i,1}} \\\\\nCL_i &= tvCL \\cdot e^{Œ∑_{i,2}} \\\\\nVc_i &= tvVc \\cdot e^{Œ∑_{i,3}}\n\\end{align*}\\]\nDynamics ‚Üê This lecture focuses here \\[\n\\begin{align*}\n\\frac{dDepot(t)}{dt} =&  - Ka \\cdot Depot(t) \\\\\n\\frac{dCentral(t)}{dt} =& Ka \\cdot Depot(t) - \\color{red}{NN(...)}\n\\end{align*}\n\\]\nError model \\[\nDV(t) \\sim Normal\\left(\\frac{Central(t)}{Vc}, \\frac{Central(t)}{Vc} \\cdot œÉ\\right)\n\\]"
  },
  {
    "objectID": "02-deepnlme-part1.html#neural-embedded-dynamical-systems",
    "href": "02-deepnlme-part1.html#neural-embedded-dynamical-systems",
    "title": "DeepNLME",
    "section": "Neural-embedded dynamical systems",
    "text": "Neural-embedded dynamical systems\n2018 - ‚ÄúNeural Ordinary Differential Equations‚Äù, Chen et al.\n\n2020 - ‚ÄúUniversal Differential Equations for Scientific Machine Learning‚Äù, Rackauckas et al.\n\n\n\nNeural ODE\n\\[\n\\frac{d\\mathbf{X}}{dt} = NN(\\mathbf{X}(t), t)\n\\]\n\n\nODE solver as scaffold for neural networks\nSimilar to recurrent neural networks and ResNets\nPure machine learning approach\n\n\n\n\nUniversal Differential Equations (UDE)\n\\[\\begin{align*}\n\\frac{dx}{dt} &= x \\cdot y - NN(x)\\\\\n\\frac{dy}{dt} &= p - x \\cdot y\n\\end{align*}\\]\n\n\nInsert universal approximators (NNs) to capture unknown terms\nCombine scientific knowledge with machine learning\nMore data-efficient than pure ML approaches\n\n\n\n\n\nScientific Machine Learning (SciML)\n\n\nAbstract idea: mixing science and machine learning\nUmbrella term for hybrid approaches\nIncludes UDEs, Physics-Informed NNs, etc.\nGoal: leverage domain knowledge to improve ML"
  },
  {
    "objectID": "02-deepnlme-part1.html#encoding-knowledge",
    "href": "02-deepnlme-part1.html#encoding-knowledge",
    "title": "DeepNLME",
    "section": "Encoding Knowledge",
    "text": "Encoding Knowledge\n\n\nPure Neural ODE \\[\n\\begin{aligned}\n\\frac{dDepot}{dt} &= NN(Depot, Central, R)[1]\\\\\n\\frac{dCentral}{dt} &= NN(Depot, Central, R)[2]\\\\\n\\frac{dR}{dt} &= NN(Depot, Central, R)[3]\n\\end{aligned}\n\\]\n\nNumber of states\n\n\nGraph Neural ODE \\[\n\\begin{aligned}\n\\frac{dDepot}{dt} &= - NN_1(Depot)\\\\\n\\frac{dCentral}{dt} &= NN_1(Depot) - NN_2(Central)\\\\\n\\frac{dR}{dt} &= NN_3(Central, R)\n\\end{aligned}\n\\]\n\nNumber of states\nDependencies\n\nConservation principles\n\n\n\n\nUDE \\[\n\\begin{aligned}\n\\frac{dDepot}{dt} &= - K_a \\cdot Depot\\\\\n\\frac{dCentral}{dt} &= K_a \\cdot Depot - CL/V_c \\cdot Central\\\\\n\\frac{dR}{dt} &= NN_3\\left(\\frac{Central}{V_c}, R\\right)\n\\end{aligned}\n\\]\n\nExplicit knowledge of some terms\nNeural networks only where needed\n\n\n\nTargeted Neural Enhancement (still a UDE) \\[\n\\begin{aligned}\n\\frac{dDepot}{dt} &= - K_a \\cdot Depot\\\\\n\\frac{dCentral}{dt} &= K_a \\cdot Depot - CL/V_c \\cdot Central\\\\\n\\frac{dR}{dt} &= k_{in} \\cdot \\left(1 + NN\\left(\\frac{Central}{V_c}\\right)\\right) - k_{out} \\cdot R\n\\end{aligned}\n\\]\n\nMost knowledge encoded!\nNeural network captures unknown drug effect\nPrecise positioning and inputs"
  },
  {
    "objectID": "02-deepnlme-part1.html#extending-for-longitudinal-data-with-deepnlme",
    "href": "02-deepnlme-part1.html#extending-for-longitudinal-data-with-deepnlme",
    "title": "DeepNLME",
    "section": "Extending for longitudinal data with DeepNLME",
    "text": "Extending for longitudinal data with DeepNLME\n\\[\\begin{equation}\nŒ∑ \\sim \\mathcal{N}\\left(Œ©\\right)\n\\end{equation}\\] \\[\\begin{align*}\nKa &= tvKa \\cdot e^{Œ∑_{2}} \\\\\nV_c &= tvV_c \\cdot e^{Œ∑_{3}} \\\\\nKout &= tvKout \\cdot e^{Œ∑_{4}}\n\\end{align*}\\]\n\\[\\begin{align*}\n\\frac{\\mathrm{d} Depot(t)}{\\mathrm{d}t} &=  - Ka \\cdot Depot(t) \\\\\n\\frac{\\mathrm{d} Central(t)}{\\mathrm{d}t} &= \\frac{ - CL \\cdot Central(t)}{V_c} + Ka \\cdot Depot(t) \\\\\n\\frac{\\mathrm{d} R(t)}{\\mathrm{d}t} &= Kin \\cdot \\left( 1 + NN\\left(\\frac{Central}{V_c} \\right) \\right) - Kout \\cdot R(t)\n\\end{align*}\\]\n\\[\\begin{align*}\nyPK &\\sim \\mathrm{Normal}\\left( \\frac{Central}{V_c}, œÉ_{pk} \\right) \\\\\nyPD &\\sim \\mathrm{Normal}\\left( R, œÉ_{pd} \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "02-deepnlme-part1.html#individual-level-neural-networks",
    "href": "02-deepnlme-part1.html#individual-level-neural-networks",
    "title": "DeepNLME",
    "section": "Individual-level neural networks",
    "text": "Individual-level neural networks\n\\[\\begin{equation}\nŒ∑ \\sim \\mathcal{N}\\left(Œ©\\right)\n\\end{equation}\\] \\[\\begin{align*}\nKa &= tvKa \\cdot e^{Œ∑_{2}} \\\\\nV_c &= tvV_c \\cdot e^{Œ∑_{3}} \\\\\nKout &= tvKout \\cdot e^{Œ∑_{4}}\n\\end{align*}\\]\n\\[\\begin{align*}\n\\frac{\\mathrm{d} Depot(t)}{\\mathrm{d}t} &=  - Ka \\cdot Depot(t) \\\\\n\\frac{\\mathrm{d} Central(t)}{\\mathrm{d}t} &= \\frac{ - CL \\cdot Central(t)}{V_c} + Ka \\cdot Depot(t) \\\\\n\\frac{\\mathrm{d} R(t)}{\\mathrm{d}t} &= Kin \\cdot \\left( 1 + NN\\left(\\frac{Central}{V_c}, {\\color{orange} Œ∑‚ÇÅ} \\right) \\right) - Kout \\cdot R(t)\n\\end{align*}\\]\n\\[\\begin{align*}\nyPK &\\sim \\mathrm{Normal}\\left( \\frac{Central}{V_c}, œÉ_{pk} \\right) \\\\\nyPD &\\sim \\mathrm{Normal}\\left( R, œÉ_{pd} \\right)\n\\end{align*}\\]"
  },
  {
    "objectID": "01-introduction.html#workshop-overview",
    "href": "01-introduction.html#workshop-overview",
    "title": "DeepPumas for Viral Dynamics",
    "section": "Workshop Overview",
    "text": "Workshop Overview\n\n\n\nTime\nSession\n\n\n\n\n09:00 - 09:20\nWelcome and Introduction\n\n\n09:20 - 10:30\nNLME modeling in Pumas\n\n\n10:30 - 10:45\n‚òï Coffee Break\n\n\n10:45 - 11:15\nDeepNLME\n\n\n\nNeural networks, SciML, UDEs and NeuralODEs\n\n\n11:15 - 12:30\nDeepNLME (hands-on)\n\n\n12:30 - 13:30\nüçΩÔ∏è Lunch Break\n\n\n13:30 - 13:55\nRandom effects, fitting NLME, and Generative AI\n\n\n13:55 - 14:40\nDeepNLME with Complex Covariates\n\n\n14:40 - 15:00\nNLME and Generative AI\n\n\n15:00 - 15:30\nEmbeddings\n\n\n15:30 - 15:45\n‚òï Coffee Break\n\n\n15:45 - 16:15\nEpidemiology Demo\n\n\n16:15 - 17:00\nDiscussions and Conclusions"
  },
  {
    "objectID": "01-introduction.html#section",
    "href": "01-introduction.html#section",
    "title": "DeepPumas for Viral Dynamics",
    "section": "",
    "text": "Machine learning\n\n\n\n\nData-driven model discovery\nFinds unintuitive relationships\nHandles complex data\n\n\n\n\n\n\n\n\nLacks scientific understanding\nRequires big data\n\n\n\n\n\nScientific modelling\n\n\n\n\nEncodes scientific understanding\nData-efficient\nInterpretable\nSimple counterfactuals\n\n\n\n\n\n\n\n\nLabor intensive\nMisses unintuitive relationships\nHard to utilize complex data"
  },
  {
    "objectID": "01-introduction.html#data-knowledge",
    "href": "01-introduction.html#data-knowledge",
    "title": "DeepPumas for Viral Dynamics",
    "section": "Data + Knowledge",
    "text": "Data + Knowledge"
  },
  {
    "objectID": "01-introduction.html#vision-spanning-pharma",
    "href": "01-introduction.html#vision-spanning-pharma",
    "title": "DeepPumas for Viral Dynamics",
    "section": "Vision Spanning Pharma",
    "text": "Vision Spanning Pharma"
  },
  {
    "objectID": "01-introduction.html#traditional-nlme",
    "href": "01-introduction.html#traditional-nlme",
    "title": "DeepPumas for Viral Dynamics",
    "section": "Traditional NLME",
    "text": "Traditional NLME\n\n\nNonlinear Mixed Effects\n\n\n\n\n\n\n\n\n\n\n\n\nTypical values \\[\ntvKa, \\; tvCL, \\; tvVc, \\; Œ©, \\; œÉ\n\\]\nCovariates \\[\nAge, \\; Weight\n\\]\nRandom effects \\[\nŒ∑ \\sim MvNormal(Œ©)\n\\]\n\n\nIndividual parameters \\[\\begin{align*}\nKa_i &= tvKa \\cdot e^{Œ∑_{i,1}} \\\\\nCL_i &= tvCL \\cdot e^{Œ∑_{i,2}} \\\\\nVc_i &= tvVc \\cdot e^{Œ∑_{i,3}}\n\\end{align*}\\]\n\n\nDynamics \\[\n\\begin{align*}\n\\frac{dDepot(t)}{dt} =&  - Ka \\cdot Depot(t) \\\\\n\\frac{dCentral(t)}{dt} =& Ka \\cdot Depot(t) - \\frac{CL}{Vc} \\cdot Central(t)\n\\end{align*}\n\\]\n\n\nError model \\[\ndv(t) \\sim Normal\\left(\\frac{Central(t)}{Vc}, \\frac{Central(t)}{Vc} \\cdot œÉ\\right)\n\\]"
  },
  {
    "objectID": "01-introduction.html#lets-get-started",
    "href": "01-introduction.html#lets-get-started",
    "title": "DeepPumas for Viral Dynamics",
    "section": "Let‚Äôs Get Started!",
    "text": "Let‚Äôs Get Started!\n\nNext up: Hands-on NLME modeling with Pumas\n\nSynthetic viral dynamics data mimicking HIV\nModel building and fitting\n\nUnderstanding the foundations before we add neural networks"
  },
  {
    "objectID": "03-random-effects-revisited.html#what-are-mixed-effects-models",
    "href": "03-random-effects-revisited.html#what-are-mixed-effects-models",
    "title": "Random effects",
    "section": "What are mixed effects models?",
    "text": "What are mixed effects models?\n\nFixed effects, \\(Œ∏\\)\n\nModel parameters modelled as deterministic quantities\n\nRandom effects, \\(Œ∑_i\\)\n\nModel parameters modelled as random variables\n\n\nHierarchical\nWe typically define hierarchies where \\(Œ∏\\) are shared parameters but \\(Œ∑\\) is subject-specific."
  },
  {
    "objectID": "03-random-effects-revisited.html#no-need-to-assign-too-much-meaning-to-random-effects",
    "href": "03-random-effects-revisited.html#no-need-to-assign-too-much-meaning-to-random-effects",
    "title": "Random effects",
    "section": "No need to assign too much meaning to random effects",
    "text": "No need to assign too much meaning to random effects\n\nIndicates unknown parameters that vary between subjects (or whatever hierarchy we use)\nUsually tied very closely to a specific parameter in pharmacometrics. \\(CL = tvCL \\cdot e^{Œ∑_{CL}}\\)\nEnables degree o freedom along which the model can account for heterogenous outcomes"
  },
  {
    "objectID": "03-random-effects-revisited.html#simulating-with-random-effects",
    "href": "03-random-effects-revisited.html#simulating-with-random-effects",
    "title": "Random effects",
    "section": "Simulating with random effects",
    "text": "Simulating with random effects\nSimple\n\nWe have given \\(Œ∏\\) and covariates \\(x\\)\nSample from the prior \\(Œ∑ | Œ∏\\)\nCompute your individual parameters and propagate your ODE\nSample your observations \\(y | Œ∏, Œ∑, x\\)"
  },
  {
    "objectID": "03-random-effects-revisited.html#fitting-with-random-effects",
    "href": "03-random-effects-revisited.html#fitting-with-random-effects",
    "title": "Random effects",
    "section": "Fitting with random effects",
    "text": "Fitting with random effects\nConditional probability / Joint likelihood / MLE\nProbability of the response \\(y\\) according to the model given specific values of \\(Œ∏\\), \\(Œ∑\\), and \\(x\\).\n\\[\np_c(y | Œ∏, Œ∑, x)\n\\]\nFit model by simply finding the values of \\(Œ∏\\) and \\(Œ∑\\) that jointly maximize the probability?\nEquivalent to minimizing a distance metric (e.g.¬†MSE) between observed and predicted data.\n\nNot what we do"
  },
  {
    "objectID": "03-random-effects-revisited.html#fitting-with-random-effects-1",
    "href": "03-random-effects-revisited.html#fitting-with-random-effects-1",
    "title": "Random effects",
    "section": "Fitting with random effects",
    "text": "Fitting with random effects\nMarginal probability\nIntegrates out the effect of the random effects.\n\\[\np_m(y | Œ∏, x) = \\int p_c(y | Œ∏, Œ∑, x) \\cdot p_{prior}(Œ∑ | Œ∏) dŒ∑\n\\]\nAverage conditional probability weighted by a prior"
  },
  {
    "objectID": "03-random-effects-revisited.html#fitting-with-random-effects-2",
    "href": "03-random-effects-revisited.html#fitting-with-random-effects-2",
    "title": "Random effects",
    "section": "Fitting with random effects",
    "text": "Fitting with random effects"
  },
  {
    "objectID": "03-random-effects-revisited.html#one-dimension-per-random-effect",
    "href": "03-random-effects-revisited.html#one-dimension-per-random-effect",
    "title": "Random effects",
    "section": "One dimension per random effect",
    "text": "One dimension per random effect\n\n\\(Œ∑\\) here can be multi-dimensional\n\n\\[\np_m(y | Œ∏, x) = \\int p_c(y | Œ∏, Œ∑, x) \\cdot p_{prior}(Œ∑ | Œ∏) dŒ∑\n\\]\n\nThis is a multi-variate integral\nOne dimension (degree of freedom) along which to account for between subject variability in the data for each random effect.\nMarginalization incentivizes that each random effect controls a single smooth dimension between-subject variability."
  },
  {
    "objectID": "03-random-effects-revisited.html#smoothness",
    "href": "03-random-effects-revisited.html#smoothness",
    "title": "Random effects",
    "section": "‚ÄúSmoothness‚Äù?",
    "text": "‚ÄúSmoothness‚Äù?\n\n\nClassical NLME \\[\nEFF = \\left(1 + Smax \\cdot \\frac{C}{tvSC50 \\cdot exp\\left(\\mathbf{\\eta}\\right) + C}\\right)\n\\]\n\nThis function is somewhat smooth in \\(Œ∑\\) by structural definition.\nVery little flexibility to affect the smoothness by tuning fixed effects.\n\n\n\n\nDeepNLME\n\\[\nEFF = \\left(1 + NN(C, Œ∑)\\right)\n\\]\n\nThis function can be very non-smooth in \\(Œ∑\\).\nLots of flexibility to affect the smoothness by tuning fixed effects.\nIncentivizing smoothness by marginalization in the fit really helps here!"
  },
  {
    "objectID": "03-random-effects-revisited.html#smoothness-in-deepnlme",
    "href": "03-random-effects-revisited.html#smoothness-in-deepnlme",
    "title": "Random effects",
    "section": "Smoothness in DeepNLME",
    "text": "Smoothness in DeepNLME\n\n\nData-generating function: \\[\nY = \\frac{E_{max} \\cdot x}{EC_{50} + x} + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n\\]\nwhere \\[\n\\begin{align}\nE_{max} &\\sim \\mathcal{U}(0.5, 1.5) \\\\\nEC_{50} &\\sim \\mathrm{LogNormal}(-2, 1.0)\n\\end{align}\n\\]\nDeepNLME model: \\[\n\\begin{align}\nY &= {\\color{orange}NN(x, Œ∑‚ÇÅ, Œ∑‚ÇÇ)} + \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\\\\\nŒ∑ &\\sim \\mathcal{N}(0, I)\n\\end{align}\n\\]"
  },
  {
    "objectID": "05-covariate-modelling.html#augment-workflow",
    "href": "05-covariate-modelling.html#augment-workflow",
    "title": "Complex covariates",
    "section": "Augment workflow",
    "text": "Augment workflow\n\n\nGreen: observed quantities\nBlue: Fixed effects\nRed: Random effects"
  },
  {
    "objectID": "05-covariate-modelling.html#covariate-free-model",
    "href": "05-covariate-modelling.html#covariate-free-model",
    "title": "Complex covariates",
    "section": "Covariate-free model",
    "text": "Covariate-free model\n\n\nSo far, our models have been covariate-free\nThey cannot exploit known heterogeneity in the patients to give better predictions\n\nCan we predict the values of the random effect from the covariates?"
  },
  {
    "objectID": "05-covariate-modelling.html#supervised-learning",
    "href": "05-covariate-modelling.html#supervised-learning",
    "title": "Complex covariates",
    "section": "Supervised learning",
    "text": "Supervised learning"
  },
  {
    "objectID": "05-covariate-modelling.html#augmented-model",
    "href": "05-covariate-modelling.html#augmented-model",
    "title": "Complex covariates",
    "section": "Augmented model",
    "text": "Augmented model"
  },
  {
    "objectID": "05-covariate-modelling.html#to-make-it-harder",
    "href": "05-covariate-modelling.html#to-make-it-harder",
    "title": "Complex covariates",
    "section": "To make it harder‚Ä¶",
    "text": "To make it harder‚Ä¶\n\nWe could use EBEs directly - in DeepPumas, we don‚Äôt\nWe can use transformed EBEs - that we do\n\northogonalized MvNormals\nstandardized to similar absolute values\nunconstrained values\ninvertible\nWe can weight with sensitivity of the loglikelihood\n\nWe could use the ‚Äúfull‚Äù posterior distribution"
  },
  {
    "objectID": "05-covariate-modelling.html#what-were-really-after",
    "href": "05-covariate-modelling.html#what-were-really-after",
    "title": "Complex covariates",
    "section": "What we‚Äôre really after",
    "text": "What we‚Äôre really after\n\\[\n\\eta_i \\sim p(\\eta | \\text{covariates})\n\\]\nA rich description of how covariate information affects the prior distribution of the random effects.\nTo be presented at ACoP in two weeks"
  },
  {
    "objectID": "07-epidemiology-demo.html#setup",
    "href": "07-epidemiology-demo.html#setup",
    "title": "Epidemiology Demo",
    "section": "Setup",
    "text": "Setup"
  },
  {
    "objectID": "07-epidemiology-demo.html#the-problem-age-stratified-seroprevalence",
    "href": "07-epidemiology-demo.html#the-problem-age-stratified-seroprevalence",
    "title": "Epidemiology Demo",
    "section": "The Problem: Age-Stratified Seroprevalence",
    "text": "The Problem: Age-Stratified Seroprevalence"
  },
  {
    "objectID": "07-epidemiology-demo.html#the-challenge",
    "href": "07-epidemiology-demo.html#the-challenge",
    "title": "Epidemiology Demo",
    "section": "The Challenge",
    "text": "The Challenge\n\nMulti-country seroprevalence data across age groups\nUnknown force of infection Œª(age) patterns\n\nHeterogeneous data quality: Some countries well-sampled, others sparse\nNeed to share information between heterogeneous countries\nDiscover age-specific transmission patterns\n\nDeepNLME approach: Let data discover the functional form!"
  },
  {
    "objectID": "07-epidemiology-demo.html#deepnlme-epidemiological-model",
    "href": "07-epidemiology-demo.html#deepnlme-epidemiological-model",
    "title": "Epidemiology Demo",
    "section": "DeepNLME Epidemiological Model",
    "text": "DeepNLME Epidemiological Model\n\nepi_model = @model begin\n  @param begin\n    N ‚àà RealDomain(; lower=0, init=1000)\n    Œª ‚àà MLPDomain(2, 5, 5, (1, softplus); reg=L2(1e-2))\n  end\n  @random Œ∑ ~ Normal(0, 0.1)\n  @init S = 1\n  @dynamics begin\n    S' = - Œª(t/100, Œ∑)[1] * S\n  end\n  @derived PosFrac ~ @. Beta(abs(N*(1-0.99S)), abs(N*(0.99S)))\nend\n\nSIR model, except that we don‚Äôt need the I or R here.\nKey features:\n\nNeural network Œª discovers age-specific force of infection\nIndividual random effects Œ∑ allow country-specific scaling\nShared function across populations with individual tuning\nMechanistic structure preserves epidemiological interpretation"
  },
  {
    "objectID": "07-epidemiology-demo.html#model-predictions",
    "href": "07-epidemiology-demo.html#model-predictions",
    "title": "Epidemiology Demo",
    "section": "Model Predictions",
    "text": "Model Predictions\n\nExcellent fit across all countries, including sparsely sampled test data!"
  },
  {
    "objectID": "07-epidemiology-demo.html#comparison-with-truth",
    "href": "07-epidemiology-demo.html#comparison-with-truth",
    "title": "Epidemiology Demo",
    "section": "Comparison with Truth",
    "text": "Comparison with Truth\n\nNeural network accurately recovers the true underlying force of infection!"
  },
  {
    "objectID": "07-epidemiology-demo.html#discovered-force-of-infection-pattern",
    "href": "07-epidemiology-demo.html#discovered-force-of-infection-pattern",
    "title": "Epidemiology Demo",
    "section": "Discovered Force of Infection Pattern",
    "text": "Discovered Force of Infection Pattern\n\nComplex multi-modal pattern discovered automatically from data!"
  },
  {
    "objectID": "07-epidemiology-demo.html#information-sharing-across-populations",
    "href": "07-epidemiology-demo.html#information-sharing-across-populations",
    "title": "Epidemiology Demo",
    "section": "Information Sharing Across Populations",
    "text": "Information Sharing Across Populations\n\nDense data informs sparse data through shared neural network function!"
  },
  {
    "objectID": "07-epidemiology-demo.html#what-deepnlme-achieved",
    "href": "07-epidemiology-demo.html#what-deepnlme-achieved",
    "title": "Epidemiology Demo",
    "section": "What DeepNLME Achieved",
    "text": "What DeepNLME Achieved\n\nData-driven discovery of force of infection Œª(age)\nAutomatic pattern recognition without pre-specifying functional forms\n\nInformation sharing\nIndividual adaptation / posterior predictions\nRobust extrapolation to poorly sampled regions/countries"
  }
]